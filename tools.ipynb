{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webutils import get_title_url_year\n",
    "# 文章的google scholar 链接\n",
    "url_scholar = 'https://scholar.google.com/scholar?cites=10525391200502590326&as_sdt=2005&sciodt=0,5&hl=zh-CN'\n",
    "get_title_url_year(url_scholar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import Paper\n",
    "\n",
    "# 读取检索到的所有文章的title，url，year\n",
    "df = pd.read_csv('data/all_articles.csv')\n",
    "papers = []\n",
    "for index, row in df.iterrows():\n",
    "    title = row[0]\n",
    "    url = row[1]\n",
    "    year = row[2]\n",
    "    papers.append(Paper(title, url, year=year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webutils import arxiv,nips,acl,openreview,ieee\n",
    "import csv\n",
    "\n",
    "arxiv([p for p in papers if 'arxiv' in p.url])\n",
    "nips([p for p in papers if 'neurips' in p.url])\n",
    "acl([p for p in papers if 'aclanthology' in p.url])\n",
    "openreview([p for p in papers if 'openreview' in p.url])\n",
    "ieee([p for p in papers if 'ieee.org' in p.url])\n",
    "\n",
    "seen = set()\n",
    "with open(\"data/papers_with_author.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Title\", \"URL\", \"Authors\", \"year\"])\n",
    "    keywords = {'arxiv', 'neurips', 'aclanthology', 'openreview', 'ieee.org'}\n",
    "    for paper in (p for p in papers if any(keyword in p.url for keyword in keywords)):\n",
    "        author_list = []\n",
    "        for author in paper.authors:\n",
    "            if author not in seen:\n",
    "                seen.add(author)\n",
    "                author_list.append(author)\n",
    "        writer.writerow([paper.title, paper.url, author_list, paper.year])\n",
    "        # 保存'arxiv', 'neurips', 'aclanthology', 'openreview', 'ieee.org' 的相关信息,其他出处文章请自行查找并加入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from homepage_utils import get_author_homepage\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('data/combined_result.csv')\n",
    "papers = []\n",
    "df = df[400:]\n",
    "print(len(df))\n",
    "for index, row in df.iterrows():\n",
    "    title = row[0]\n",
    "    url = row[1]\n",
    "    str = row[2]\n",
    "    year = row[3]\n",
    "    author_list = ast.literal_eval(str)\n",
    "    print(author_list)\n",
    "    papers.append(Paper(title, url, authors=author_list, year=year))\n",
    "for paper in tqdm(papers):\n",
    "    for author in paper.authors:\n",
    "        homepage = get_author_homepage(author,api_key=\"75dc32c067fba13343b03a6b93fae6ef1b461397\")\n",
    "        print(homepage)\n",
    "        paper.author_homepages.append(homepage)\n",
    "with open(\"data/papers_with_homepage5.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Title\", \"URL\", \"Authors\", \"Authors-HomePage\", \"Year-Source\"])\n",
    "    for paper in [p for p in papers]:\n",
    "        writer.writerow([paper.title, paper.url, paper.authors, paper.author_homepages, paper.year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的cell需要自己找文章的引用标号，并提前下载pdf文件，可以先行跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from find_citation import extract_text_with_fitz,find_citation_symbol,find_content,find_citing_sentences\n",
    "from utils import read_data\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "keywords = {'Yu et al.',\n",
    "            'Yu et al., 2023','Yu et al.,2023','Yu et al.,\\n2023','Yu et al.,\\n 2023','Yu et al., \\n2023', 'Yu et al., 2024','Yu et al.,2024','Yu et al.,\\n2024','Yu et al.,\\n 2024','Yu et al., \\n2024',\n",
    "            'Yu et al., (2023)','Yu et al.,(2023)','Yu et al.,\\n(2023)','Yu et al.,\\n (2023)','Yu et al., \\n(2023)', 'Yu et al., (2024)','Yu et al.,(2024)','Yu et al.,\\n(2024)','Yu et al.,\\n (2024)','Yu et al., \\n(2024)',\n",
    "            'Yu et al. 2023','Yu et al.2023','Yu et al.\\n2023','Yu et al.\\n 2023','Yu et al. \\n2023', 'Yu et al. 2024','Yu et al.2024','Yu et al.\\n2024','Yu et al.\\n 2024','Yu et al. \\n2024',\n",
    "            'Yu et al. (2023)','Yu et al.(2023)','Yu et al.\\n(2023)','Yu et al.\\n (2023)','Yu et al. \\n(2023)', 'Yu et al. (2024)','Yu et al.(2024)','Yu et al.\\n(2024)','Yu et al.\\n (2024)','Yu et al. \\n(2024)',             \n",
    "            }\n",
    "\n",
    "papers = read_data('./data/papers_with_homepage.csv')\n",
    "os.makedirs('./downloads',exist_ok=True)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "for paper in papers:\n",
    "    if 'arxiv' not in paper.url:\n",
    "        continue\n",
    "    arxiv_id = paper.url.split('/')[-1]\n",
    "    arxiv_pdf = os.path.join('downloads',f'{arxiv_id}.pdf')\n",
    "    text = extract_text_with_fitz(arxiv_pdf)\n",
    "    content = find_content(text,keywords)\n",
    "    symbol = find_citation_symbol(content,keywords)\n",
    "    # 如果Yu et al. 这种方式没有找到，则进入标号匹配[34]模式\n",
    "    if content == '未找到':\n",
    "        title = 'Metamath' # 此处替换成自己文章中的关键字\n",
    "        print(arxiv_pdf)\n",
    "        sentences,symbol_number = find_citing_sentences(text, title)\n",
    "        if sentences:\n",
    "           print('标号模式找到原文')\n",
    "           content = sentences[0] \n",
    "        if symbol_number:\n",
    "            symbol = str(symbol_number)\n",
    "\n",
    "    paper.set_content(content)\n",
    "    paper.set_symbol(symbol)\n",
    "\n",
    "with open(\"data/papers_with_content.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Title\", \"URL\", \"Authors\", \"Authors-HomePage\", \"Year-Source\", \"Content\", \"Symbol\"])\n",
    "    for paper in [p for p in papers]:\n",
    "        writer.writerow([paper.title, paper.url, paper.authors, paper.author_homepages, paper.year, paper.content, paper.symbol])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptutils import generate\n",
    "from utils import read_data\n",
    "\n",
    "\n",
    "papers = read_data('./data/papers_with_homepage.csv')\n",
    "generate(papers,'./data/test.pptx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
